{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notifications through IFTTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def send_notification(message):\n",
    "    IFTTT_WEBHOOKS_URL = 'https://maker.ifttt.com/trigger/PyCheckpoint/with/key/c3GzUla4l68wv5XZaYpdMK'\n",
    "    data = {'value1': message}\n",
    "    #ifttt_event_url = IFTTT_WEBHOOKS_URL.format(event)\n",
    "    requests.post(IFTTT_WEBHOOKS_URL, json=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns = 118\n",
      "Number of rows = 65540\n"
     ]
    }
   ],
   "source": [
    "# Importing the file\n",
    "parcelMain = pd.read_csv('ParcelData_Enriched_HistoryJoined.csv', low_memory=False)\n",
    "print(\"Number of columns = \"+str(len(parcelMain.columns)))\n",
    "print(\"Number of rows = \"+str(len(parcelMain)))\n",
    "\n",
    "# Marking all irrelevant fields in the csv\n",
    "# Indexes are 0 to n-1\n",
    "irrelevantFields = [0,1,7,23,24,25,38,43,44,45,46,47,48,49,50,51,52,53,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,104,107]\n",
    "classCodes = [81,82,83,84,85,86,87,88]\n",
    "assessments = [89,90,91,92,93,94,95,96]\n",
    "vacantFlags = [97,98,99,100,101]\n",
    "roomCount = [109]\n",
    "assessmentDouble = [114]\n",
    "\n",
    "demographics = [26,27,28,29,30,31,32,33,34,35,36,37]\n",
    "yToPredict = [115]\n",
    "XYCoords = [116,117]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all parcels that were vacant at any point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vacant lots/building codes used by the city\n",
    "vs = [300,311,312,314,315,320, 321, 322, 323, 330, 331, 340, 341, 350, 351, 352, 380, 399, 313, 316, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2950 rows\n"
     ]
    }
   ],
   "source": [
    "from math import isnan\n",
    "nvParcels = parcelMain\n",
    "classes = ['Class1990','Class1996','Class2000','Class2004','Class2008','Class2011','Class2014','Class2018']\n",
    "indexes = []\n",
    "ind = 0\n",
    "for ix, row in nvParcels.iterrows():\n",
    "    rowClass = []\n",
    "    for clx in classes:\n",
    "        rowClass.append(np.isnan(row[clx]) or int(row[clx]) in vs)\n",
    "    if all(rowClass):\n",
    "        indexes.append(ix)\n",
    "nvParcels.drop(indexes, inplace=True)\n",
    "print(\"Dropped \"+str(len(indexes))+\" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop irrelevant columns\n",
    "\n",
    "Note: Column indices will have changed after running the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of remaining columns = 53\n",
      "Number of remaining rows = 61578\n",
      "propCrimeCount                  int64\n",
      "violCrime                       int64\n",
      "allVacantSqFt                 float64\n",
      "nonIndVacantSqFt              float64\n",
      "nonIndComVacantSqFt           float64\n",
      "PropCrime2012Count              int64\n",
      "PropCrime2013Count              int64\n",
      "PropCrime2014Count              int64\n",
      "PropCrime2015Count              int64\n",
      "PropCrime2016Count              int64\n",
      "PropCrime2017Count              int64\n",
      "ViolentCrime2012Count           int64\n",
      "ViolentCrime2013Count           int64\n",
      "ViolentCrime2014Count           int64\n",
      "ViolentCrime2015Count           int64\n",
      "ViolentCrime2016Count           int64\n",
      "ViolentCrime2017Count           int64\n",
      "parcelAreaSqFt                float64\n",
      "propertyCount                   int64\n",
      "TotalCrime                      int64\n",
      "TOTPOP00                        int64\n",
      "TOTPOP10                        int64\n",
      "TOTPOP_CY                       int64\n",
      "DIVINDX_CY                    float64\n",
      "TOTHH00                         int64\n",
      "TOTHH10                         int64\n",
      "TOTHH_CY                        int64\n",
      "AVGHINC_CY                      int64\n",
      "POPGRW10CY                    float64\n",
      "BACHDEG_CY                      int64\n",
      "GRADDEG_CY                      int64\n",
      "EDUCBASECY                      int64\n",
      "SchoolDist                    float64\n",
      "LibraryDist                   float64\n",
      "ParkDistance                  float64\n",
      "nciVacantDistance             float64\n",
      "TaxParcel_LOT_FRONTAGE        float64\n",
      "TaxParcel_LOT_DEPTH           float64\n",
      "TaxParcel_STATEDAREA          float64\n",
      "TaxCombined_csv_Shape_Area    float64\n",
      "TaxCombined_csv_SqFtArea        int64\n",
      "NUMUNITS                      float64\n",
      "SQFT                          float64\n",
      "STORIES                       float64\n",
      "ROOMS                         float64\n",
      "BATHROOMS                     float64\n",
      "vacantCount_50f                 int64\n",
      "vacantCount_100f                int64\n",
      "vacantCount_150f                int64\n",
      "vacantCount_200f                int64\n",
      "AssessmentChange1814          float64\n",
      "POINT_X                       float64\n",
      "POINT_Y                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# DO NOT RUN AGAIN - Unless you have reloaded the csv file.\n",
    "# Dropping irrelevant columns\n",
    "irx = irrelevantFields+classCodes+assessments+vacantFlags+roomCount+assessmentDouble\n",
    "idx = np.r_[irx]\n",
    "nvParcels.drop(nvParcels.columns[idx], axis=1, inplace=True)\n",
    "nvParcels.dropna(inplace=True)\n",
    "print(\"Number of remaining columns = \"+str(len(nvParcels.columns)))\n",
    "print(\"Number of remaining rows = \"+str(len(nvParcels)))\n",
    "print(nvParcels.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prelim Configs\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of original data\n",
    "xOrig = nvParcels.iloc[:,0:-3].values\n",
    "yOrig = nvParcels.loc[:,'AssessmentChange1814'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50\n"
     ]
    }
   ],
   "source": [
    "# For future reference\n",
    "print(\"Number of features: \"+str(len(xOrig[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all data between -1,1\n",
    "sc2 = MinMaxScaler(copy=True, feature_range=(-1,1))\n",
    "y = sc2.fit_transform(yOrig.reshape(-1,1))\n",
    "x = sc2.transform(xOrig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for plotting errors/history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "## multiclass or binary report\n",
    "## If binary (sigmoid output), set binary parameter to True\n",
    "def full_multiclass_report(model,\n",
    "                           x,\n",
    "                           y_true,\n",
    "                           classes,\n",
    "                           batch_size=32,\n",
    "                           binary=False):\n",
    "\n",
    "    # 1. Transform one-hot encoded y_true into their class number\n",
    "    if not binary:\n",
    "        y_true = np.argmax(y_true,axis=1)\n",
    "    \n",
    "    # 2. Predict classes and stores in y_pred\n",
    "    y_pred = model.predict_classes(x, batch_size=batch_size)\n",
    "    \n",
    "    # 3. Print accuracy score\n",
    "    print(\"Accuracy : \"+ str(accuracy_score(y_true,y_pred)))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # 4. Print classification report\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true,y_pred,digits=5))    \n",
    "    \n",
    "    # 5. Plot confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    print(cnf_matrix)\n",
    "    plot_confusion_matrix(cnf_matrix,classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "parameters = {\n",
    "        'batch_size': [100],\n",
    "        'epochs': [500],\n",
    "        'optimizer': ['adam'],\n",
    "        'activation': ['tanh']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold\n",
    "kf = KFold(n_splits=5)\n",
    "history = {}\n",
    "trainNumber = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE TRAINING\n",
    "\n",
    "#Logging using tensorboard\n",
    "for batch_size in parameters['batch_size']:\n",
    "    for epochs in parameters['epochs']:\n",
    "        for optimizer in parameters['optimizer']:\n",
    "            for activation in parameters['activation']:\n",
    "                trainErrors = []\n",
    "                testErrors = []\n",
    "                for i in range(0,5):\n",
    "                    regressor = Sequential()\n",
    "                    for k in range(5,51,5):\n",
    "                        regressor.add(Dense(k, kernel_initializer='uniform', activation=activation, input_dim=len(xOrig[0])))\n",
    "                        for j in range(0,i):\n",
    "                            regressor.add(Dense(k, kernel_initializer='uniform', activation=activation))\n",
    "                        regressor.add(Dense(1, kernel_initializer='uniform', activation=activation))\n",
    "                        regressor.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "                        for train_index, test_index in kf.split(x):\n",
    "                            xTrain, xTest = x[train_index], x[test_index]\n",
    "                            yTrain, yTest = y[train_index], y[test_index]\n",
    "                            print(\"Running iteration \"+str(trainNumber+1))\n",
    "                            tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "                            history[trainNumber] = {}\n",
    "                            history[trainNumber]['model'] = regressor.fit(xTrain, yTrain, epochs = epochs, batch_size = batch_size, verbose=0, validation_data=(xTest,yTest), callbacks=[tensorboard])\n",
    "                            history[trainNumber]['params'] = {\n",
    "                                'batch_size': batch_size,\n",
    "                                'epochs': epochs,\n",
    "                                'optimizer': optimizer,\n",
    "                                'activation': activation,\n",
    "                                'hidden-layers': i,\n",
    "                                'nodes': k\n",
    "                            }\n",
    "                            trainNumber+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## BEWARE: TAKES A LOT OF TIME TO RUN (~ Weeks)\n",
    "##\n",
    "\n",
    "from math import ceil\n",
    "import json\n",
    "\n",
    "# Hyperparameters\n",
    "parameters = {\n",
    "    'batch_size': [100],\n",
    "    'epochs': [500],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'activation': ['linear', 'tanh', 'relu']\n",
    "}\n",
    "\n",
    "# History log\n",
    "history = {}\n",
    "trainNumber = 0\n",
    "modelHistory = {}\n",
    "model = \"\"\n",
    "try:\n",
    "    def createmodel(num_nodes, activation, optimizer, input_dim):\n",
    "        global model\n",
    "        model = Sequential()\n",
    "        if num_nodes < 50:\n",
    "            model.add(Dense(num_nodes+1, kernel_initializer='uniform', activation=activation, input_dim=input_dim))\n",
    "        elif num_nodes < 100:\n",
    "            model.add(Dense(50, kernel_initializer='uniform', activation=activation, input_dim=input_dim))\n",
    "            model.add(Dense(num_nodes%50+1, kernel_initializer='uniform', activation=activation))\n",
    "        elif num_nodes < 150:\n",
    "            model.add(Dense(50, kernel_initializer='uniform', activation=activation, input_dim=input_dim))\n",
    "            model.add(Dense(50, kernel_initializer='uniform', activation=activation))\n",
    "            model.add(Dense(num_nodes%100+1, kernel_initializer='uniform', activation=activation))\n",
    "        model.add(Dense(1, kernel_initializer='uniform', activation=activation))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse', 'mae', 'mape', 'cosine'])\n",
    "\n",
    "    # THE TRAINING\n",
    "    send_notification(\"Starting Training...\")\n",
    "    for batch_size in parameters['batch_size']:\n",
    "        if batch_size not in history:\n",
    "            history[batch_size] = {}\n",
    "\n",
    "        for epochs in parameters['epochs']:\n",
    "            if epochs not in history[batch_size]:\n",
    "                history[batch_size][epochs] = {}\n",
    "\n",
    "            for optimizer in parameters['optimizer']:\n",
    "                if optimizer not in history[batch_size][epochs]:\n",
    "                    history[batch_size][epochs][optimizer] = {}\n",
    "\n",
    "                for activation in parameters['activation']:\n",
    "                    if activation not in history[batch_size][epochs][optimizer]:\n",
    "                        history[batch_size][epochs][optimizer][activation] = {}\n",
    "\n",
    "                    for growth_index in range(0,150):\n",
    "                        createmodel(growth_index, activation, optimizer, len(xOrig[0]))\n",
    "\n",
    "                        layers = int(ceil((growth_index+1) / 50.0))\n",
    "                        \n",
    "                        layer_encoded = str(layers)+\"_\"+str((growth_index%50)+1)\n",
    "                        \n",
    "                        if layer_encoded not in history[batch_size][epochs][optimizer]:\n",
    "                            history[batch_size][epochs][optimizer][activation][layer_encoded] = {}\n",
    "                        \n",
    "                        kf = KFold(n_splits=10)\n",
    "\n",
    "                        for train_index, test_index in kf.split(x):\n",
    "                            xTrain, xTest = x[train_index], x[test_index]\n",
    "                            yTrain, yTest = y[train_index], y[test_index]\n",
    "                            print(\"Running iteration \"+str(trainNumber+1))\n",
    "                            history[batch_size][epochs][optimizer][activation][layer_encoded][trainNumber] = {}\n",
    "                            \n",
    "                            # Logging\n",
    "                            LOG_DIR = \"growthLogs/batch_\"+str(batch_size)+\"/epoch_\"+str(epochs)+\"/optimizer_\"+str(optimizer)+\"/epoch_\"+str(activation)+\"/activation_\"+str(batch_size)+\"/Layers_\"+str(layers)+\"_nodes_\"+str((growth_index%50)+1)+\"/{}\"\n",
    "                            tensorboard = TensorBoard(log_dir=LOG_DIR.format(time()))\n",
    "                            \n",
    "                            modelHistory[trainNumber] = model.fit(xTrain, yTrain, epochs = epochs, batch_size = batch_size, verbose=0, validation_data=(xTest,yTest), callbacks=[tensorboard])\n",
    "                            for key in modelHistory[trainNumber].history.keys():\n",
    "                                history[batch_size][epochs][optimizer][activation][layer_encoded][trainNumber][key] = modelHistory[trainNumber].history[key]\n",
    "                            try:\n",
    "                                history[batch_size][epochs][optimizer][activation][layer_encoded][trainNumber]['params'] = {\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'epochs': epochs,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'activation': activation,\n",
    "                                    'layers': layers,\n",
    "                                    'nodes': (growth_index%50)+1\n",
    "                                }\n",
    "                            except Exception as e:\n",
    "                                print(\"Not saved\")\n",
    "                                print(e)\n",
    "                                raise\n",
    "                            try:\n",
    "                                with open('data.json', 'w') as outfile:\n",
    "                                    json.dump(history, outfile)\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                print(\"File not saved\")\n",
    "                                raise\n",
    "                            trainNumber+=1\n",
    "                    send_notification(\"Training step \"+str(trainNumber+1)+\"...\")\n",
    "    send_notification(\"Training has completed...\")\n",
    "except Exception as e:\n",
    "    send_notification(\"Training has crashed...\")\n",
    "    print(e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Less Intensive Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 1\n",
      "Running iteration 2\n",
      "Running iteration 3\n",
      "Running iteration 4\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import json\n",
    "\n",
    "# Hyperparameters\n",
    "parameters = {\n",
    "    'batch_size': [100],\n",
    "    'epochs': [500],\n",
    "    'optimizer': ['adam'],\n",
    "    'activation': ['tanh']\n",
    "}\n",
    "\n",
    "# History log\n",
    "history = {}\n",
    "trainNumber = 0\n",
    "modelHistory = {}\n",
    "model = \"\"\n",
    "try:\n",
    "    def createmodel(num_nodes, activation, optimizer, input_dim):\n",
    "        global model\n",
    "        model = Sequential()\n",
    "        # 1 layer with 1 to 5 nodes\n",
    "        if num_nodes < 5:\n",
    "            model.add(Dense(num_nodes+1, kernel_initializer='uniform', activation=activation, input_dim=input_dim))\n",
    "        model.add(Dense(1, kernel_initializer='uniform', activation=activation))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse', 'mae', 'mape', 'cosine'])\n",
    "\n",
    "    # THE TRAINING\n",
    "    send_notification(\"Starting Training...\")\n",
    "    for batch_size in parameters['batch_size']:\n",
    "        if batch_size not in history:\n",
    "            history[batch_size] = {}\n",
    "\n",
    "        for epochs in parameters['epochs']:\n",
    "            if epochs not in history[batch_size]:\n",
    "                history[batch_size][epochs] = {}\n",
    "\n",
    "            for optimizer in parameters['optimizer']:\n",
    "                if optimizer not in history[batch_size][epochs]:\n",
    "                    history[batch_size][epochs][optimizer] = {}\n",
    "\n",
    "                for activation in parameters['activation']:\n",
    "                    if activation not in history[batch_size][epochs][optimizer]:\n",
    "                        history[batch_size][epochs][optimizer][activation] = {}\n",
    "\n",
    "                    for growth_index in range(0,5):\n",
    "                        createmodel(growth_index, activation, optimizer, len(xOrig[0]))\n",
    "\n",
    "                        #layers = int(ceil((growth_index+1) / 50.0))\n",
    "                        \n",
    "                        #layer_encoded = str(layers)+\"_\"+str((growth_index%50)+1)\n",
    "                        layer_encoded = \"1_\"+str(growth_index+1)\n",
    "                        \n",
    "                        if layer_encoded not in history[batch_size][epochs][optimizer]:\n",
    "                            history[batch_size][epochs][optimizer][activation][layer_encoded] = {}\n",
    "                        \n",
    "                        kf = KFold(n_splits=5)\n",
    "\n",
    "                        for train_index, test_index in kf.split(x):\n",
    "                            xTrain, xTest = x[train_index], x[test_index]\n",
    "                            yTrain, yTest = y[train_index], y[test_index]\n",
    "                            print(\"Running iteration \"+str(trainNumber+1))\n",
    "                            history[batch_size][epochs][optimizer][activation][layer_encoded][trainNumber] = {}\n",
    "                            \n",
    "                            # Logging\n",
    "                            LOG_DIR = \"growthLogs_low/batch_\"+str(batch_size)+\"/epoch_\"+str(epochs)+\"/optimizer_\"+str(optimizer)+\"/epoch_\"+str(activation)+\"/activation_\"+str(batch_size)+\"/Layers_1_nodes_\"+str(growth_index+1)+\"/{}\"\n",
    "                            tensorboard = TensorBoard(log_dir=LOG_DIR.format(time()))\n",
    "                            \n",
    "                            modelHistory[trainNumber] = model.fit(xTrain, yTrain, epochs = epochs, batch_size = batch_size, verbose=0, validation_data=(xTest,yTest), callbacks=[tensorboard])\n",
    "                            for key in modelHistory[trainNumber].history.keys():\n",
    "                                history[batch_size][epochs][optimizer][activation][layer_encoded][trainNumber][key] = modelHistory[trainNumber].history[key]\n",
    "                            try:\n",
    "                                history[batch_size][epochs][optimizer][activation][layer_encoded][trainNumber]['params'] = {\n",
    "                                    'batch_size': batch_size,\n",
    "                                    'epochs': epochs,\n",
    "                                    'optimizer': optimizer,\n",
    "                                    'activation': activation,\n",
    "                                    'layers': 1,\n",
    "                                    'nodes': growth_index+1\n",
    "                                }\n",
    "                            except Exception as e:\n",
    "                                print(\"Not saved\")\n",
    "                                print(e)\n",
    "                                raise\n",
    "                            try:\n",
    "                                with open('data.json', 'w') as outfile:\n",
    "                                    json.dump(history, outfile)\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                print(\"File not saved\")\n",
    "                                raise\n",
    "                            trainNumber+=1\n",
    "                    send_notification(\"Training step \"+str(trainNumber+1)+\"...\")\n",
    "    send_notification(\"Training has completed...\")\n",
    "except Exception as e:\n",
    "    send_notification(\"Training has crashed...\")\n",
    "    print(e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: NN with radial basis neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radial Basis Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "\n",
    "class Radial1DTransformationLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, radial_funct, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.radial_funct = radial_funct\n",
    "        self.basis_functions = {\n",
    "            'fn1': lambda x,y : tf.exp(-tf.multiply(tf.square(x),tf.square(y))),\n",
    "            'fn2': lambda x,y : tf.sqrt(tf.add(tf.ones([1,1],dtype=tf.float32),tf.multiply(tf.square(x),tf.square(y)))),\n",
    "            'fn3': lambda x,y : tf.divide(tf.ones([1,1],tf.float32), tf.add(tf.ones([1,1],dtype=tf.float32),tf.multiply(tf.square(x),tf.square(y)))),\n",
    "            'fn4': lambda x,y : tf.divide(tf.ones([1,1], tf.float32), tf.sqrt(tf.add(tf.ones([1,1],dtype=tf.float32),tf.multiply(tf.square(x),tf.square(y)))))\n",
    "        }\n",
    "        super(Radial1DTransformationLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel', shape=(1,1), initializer='uniform', trainable=True)\n",
    "        super(Radial1DTransformationLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.basis_functions[self.radial_funct](x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neuron Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "myinp = tf.constant([1.,2.,3.,4.,5.])\n",
    "inp = Input(shape=myinp.shape)\n",
    "out = Radial1DTransformationLayer(input_shape=(1,5), output_dim=(1, 5), radial_funct='fn1')(inp)\n",
    "model = Model(input=inp, output=out)\n",
    "\n",
    "output = model.predict(myinp, steps=1)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
